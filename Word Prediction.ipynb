{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "462c409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Games\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 293ms/step - loss: 6.6840\n",
      "Epoch 2/5\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 291ms/step - loss: 5.9417\n",
      "Epoch 3/5\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 303ms/step - loss: 5.5324\n",
      "Epoch 4/5\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 294ms/step - loss: 5.1617\n",
      "Epoch 5/5\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 282ms/step - loss: 4.8980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, Text\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class TextPredictorApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Text Predictor\")\n",
    "        self.file_path = None\n",
    "        \n",
    "        self.upload_btn = tk.Button(root, text=\"Upload Dataset\", command=self.upload_file)\n",
    "        self.upload_btn.pack()\n",
    "\n",
    "        self.train_btn = tk.Button(root, text=\"Train Model\", command=self.train_model, state=tk.DISABLED)\n",
    "        self.train_btn.pack()\n",
    "\n",
    "        self.entry_label = tk.Label(root, text=\"Enter your text:\")\n",
    "        self.entry_label.pack()\n",
    "        \n",
    "        self.text_entry = tk.Entry(root, width=50)\n",
    "        self.text_entry.pack()\n",
    "\n",
    "        self.predict_btn = tk.Button(root, text=\"Predict Next Word\", command=self.predict_next_word, state=tk.DISABLED)\n",
    "        self.predict_btn.pack()\n",
    "\n",
    "        self.result_label = tk.Label(root, text=\"\")\n",
    "        self.result_label.pack()\n",
    "\n",
    "    def upload_file(self):\n",
    "        self.file_path = filedialog.askopenfilename(filetypes=[(\"Text Files\", \"*.txt\")])\n",
    "        if self.file_path:\n",
    "            self.train_btn.config(state=tk.NORMAL)\n",
    "\n",
    "    def train_model(self):\n",
    "        if self.file_path:\n",
    "            with open(self.file_path, \"r\", encoding=\"utf8\") as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            data = ' '.join(lines)\n",
    "            data = data.replace('\\n', ' ').replace('\\r', ' ').replace('\\ufeff', ' ').replace('“', ' ').replace('”', ' ')\n",
    "            data = ' '.join(data.split())\n",
    "\n",
    "            tokenizer = Tokenizer()\n",
    "            tokenizer.fit_on_texts([data])\n",
    "            self.tokenizer = tokenizer\n",
    "\n",
    "            with open('token.pkl', 'wb') as token_file:\n",
    "                pickle.dump(tokenizer, token_file)\n",
    "\n",
    "            sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "            vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "            sequences = []\n",
    "            for i in range(3, len(sequence_data)):\n",
    "                words = sequence_data[i-3:i+1]\n",
    "                sequences.append(words)\n",
    "\n",
    "            sequences = np.array(sequences)\n",
    "            X = sequences[:, :-1]\n",
    "            y = to_categorical(sequences[:, -1], num_classes=vocab_size)\n",
    "\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Embedding(vocab_size, 10, input_length=3))\n",
    "            self.model.add(LSTM(1000, return_sequences=True))\n",
    "            self.model.add(LSTM(1000))\n",
    "            self.model.add(Dense(1000, activation=\"relu\"))\n",
    "            self.model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "            self.model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "            self.model.fit(X, y, epochs=5, batch_size=64)\n",
    "            self.model.save('next_words.h5')\n",
    "\n",
    "            self.predict_btn.config(state=tk.NORMAL)\n",
    "            self.result_label.config(text=\"Model trained successfully!\")\n",
    "\n",
    "    def predict_next_word(self):\n",
    "        text = self.text_entry.get().strip()\n",
    "        if text:\n",
    "            words = text.split()[-3:]\n",
    "            sequence = self.tokenizer.texts_to_sequences([words])[0]\n",
    "            sequence = np.array([sequence])\n",
    "\n",
    "            preds = np.argmax(self.model.predict(sequence))\n",
    "            predicted_word = \"\"\n",
    "\n",
    "            for key, value in self.tokenizer.word_index.items():\n",
    "                if value == preds:\n",
    "                    predicted_word = key\n",
    "                    break\n",
    "\n",
    "            self.result_label.config(text=f\"Predicted next word: {predicted_word}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = TextPredictorApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58c0a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class ModelTrainerApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Model Trainer\")\n",
    "        self.file_path = None\n",
    "        \n",
    "        self.upload_btn = tk.Button(root, text=\"Upload Dataset\", command=self.upload_file)\n",
    "        self.upload_btn.pack()\n",
    "\n",
    "        self.train_btn = tk.Button(root, text=\"Train Model\", command=self.train_model, state=tk.DISABLED)\n",
    "        self.train_btn.pack()\n",
    "\n",
    "        self.result_label = tk.Label(root, text=\"\")\n",
    "        self.result_label.pack()\n",
    "\n",
    "    def upload_file(self):\n",
    "        self.file_path = filedialog.askopenfilename(filetypes=[(\"Text Files\", \"*.txt\")])\n",
    "        if self.file_path:\n",
    "            self.train_btn.config(state=tk.NORMAL)\n",
    "\n",
    "    def train_model(self):\n",
    "        if self.file_path:\n",
    "            with open(self.file_path, \"r\", encoding=\"utf8\") as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            data = ' '.join(lines)\n",
    "            data = data.replace('\\n', ' ').replace('\\r', ' ').replace('\\ufeff', ' ').replace('“', ' ').replace('”', ' ')\n",
    "            data = ' '.join(data.split())\n",
    "\n",
    "            tokenizer = Tokenizer()\n",
    "            tokenizer.fit_on_texts([data])\n",
    "            self.tokenizer = tokenizer\n",
    "\n",
    "            with open('token.pkl', 'wb') as token_file:\n",
    "                pickle.dump(tokenizer, token_file)\n",
    "\n",
    "            sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "            vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "            sequences = []\n",
    "            for i in range(3, len(sequence_data)):\n",
    "                words = sequence_data[i-3:i+1]\n",
    "                sequences.append(words)\n",
    "\n",
    "            sequences = np.array(sequences)\n",
    "            X = sequences[:, :-1]\n",
    "            y = to_categorical(sequences[:, -1], num_classes=vocab_size)\n",
    "\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Embedding(vocab_size, 10, input_length=3))\n",
    "            self.model.add(LSTM(1000, return_sequences=True))\n",
    "            self.model.add(LSTM(1000))\n",
    "            self.model.add(Dense(1000, activation=\"relu\"))\n",
    "            self.model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "            self.model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "            self.model.fit(X, y, epochs=70, batch_size=64)\n",
    "            self.model.save('next_words.h5')\n",
    "\n",
    "            self.result_label.config(text=\"Model trained and saved successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = ModelTrainerApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a5107b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class TextPredictorApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Text Predictor\")\n",
    "\n",
    "        self.entry_label = tk.Label(root, text=\"Enter your text:\")\n",
    "        self.entry_label.pack()\n",
    "        \n",
    "        self.text_entry = tk.Entry(root, width=50)\n",
    "        self.text_entry.pack()\n",
    "\n",
    "        self.predict_btn = tk.Button(root, text=\"Predict Next Word\", command=self.predict_next_word)\n",
    "        self.predict_btn.pack()\n",
    "\n",
    "        self.result_label = tk.Label(root, text=\"\")\n",
    "        self.result_label.pack()\n",
    "        \n",
    "        \n",
    "        self.model = load_model('next_words.h5')\n",
    "        with open('token.pkl', 'rb') as token_file:\n",
    "            self.tokenizer = pickle.load(token_file)\n",
    "\n",
    "    def predict_next_word(self):\n",
    "        text = self.text_entry.get().strip()\n",
    "        if text:\n",
    "            words = text.split()[-3:]\n",
    "            sequence = self.tokenizer.texts_to_sequences([words])[0]\n",
    "            sequence = np.array([sequence])\n",
    "\n",
    "            preds = np.argmax(self.model.predict(sequence))\n",
    "            predicted_word = \"\"\n",
    "\n",
    "            for key, value in self.tokenizer.word_index.items():\n",
    "                if value == preds:\n",
    "                    predicted_word = key\n",
    "                    break\n",
    "\n",
    "            self.result_label.config(text=f\"Predicted next word: {predicted_word}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = TextPredictorApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a30d946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from textblob import TextBlob\n",
    "\n",
    "class TextPredictorApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Text Predictor\")\n",
    "\n",
    "        self.entry_label = tk.Label(root, text=\"Enter your text:\")\n",
    "        self.entry_label.pack()\n",
    "        \n",
    "        self.text_entry = tk.Entry(root, width=50)\n",
    "        self.text_entry.pack()\n",
    "\n",
    "        self.predict_btn = tk.Button(root, text=\"Predict Next Word\", command=self.predict_next_word)\n",
    "        self.predict_btn.pack()\n",
    "\n",
    "        self.result_label = tk.Label(root, text=\"\")\n",
    "        self.result_label.pack()\n",
    "        \n",
    "        self.model = load_model('next_words.h5')\n",
    "        with open('token.pkl', 'rb') as token_file:\n",
    "            self.tokenizer = pickle.load(token_file)\n",
    "\n",
    "    def predict_next_word(self):\n",
    "        text = self.text_entry.get().strip()\n",
    "        if text:\n",
    "            words = text.split()[-3:]\n",
    "            sequence = self.tokenizer.texts_to_sequences([words])[0]\n",
    "            sequence = np.array([sequence])\n",
    "\n",
    "            preds = np.argmax(self.model.predict(sequence))\n",
    "            predicted_word = \"\"\n",
    "\n",
    "            for key, value in self.tokenizer.word_index.items():\n",
    "                if value == preds:\n",
    "                    predicted_word = key\n",
    "                    break\n",
    "\n",
    "            \n",
    "            sentiment = TextBlob(predicted_word).sentiment\n",
    "\n",
    "            \n",
    "            sentiment_label = f\"Sentiment - Polarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}\"\n",
    "            self.result_label.config(text=f\"Predicted next word: {predicted_word}\\n{sentiment_label}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = TextPredictorApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e17e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from textblob import TextBlob\n",
    "from translate import Translator\n",
    "\n",
    "class TextPredictorApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Text Predictor\")\n",
    "\n",
    "        self.entry_label = tk.Label(root, text=\"Enter your text:\")\n",
    "        self.entry_label.pack()\n",
    "        \n",
    "        self.text_entry = tk.Entry(root, width=50)\n",
    "        self.text_entry.pack()\n",
    "\n",
    "        self.predict_btn = tk.Button(root, text=\"Predict Next Word\", command=self.predict_next_word)\n",
    "        self.predict_btn.pack()\n",
    "        \n",
    "        self.translate_btn = tk.Button(root, text=\"Translate to Telugu\", command=self.translate_word, state=tk.DISABLED)\n",
    "        self.translate_btn.pack()\n",
    "\n",
    "        self.result_label = tk.Label(root, text=\"\")\n",
    "        self.result_label.pack()\n",
    "        \n",
    "        self.model = load_model('next_words.h5')\n",
    "        with open('token.pkl', 'rb') as token_file:\n",
    "            self.tokenizer = pickle.load(token_file)\n",
    "        \n",
    "        self.translator = Translator(to_lang=\"te\")\n",
    "\n",
    "    def predict_next_word(self):\n",
    "        text = self.text_entry.get().strip()\n",
    "        if text:\n",
    "            words = text.split()[-3:]\n",
    "            sequence = self.tokenizer.texts_to_sequences([words])[0]\n",
    "            sequence = np.array([sequence])\n",
    "\n",
    "            preds = np.argmax(self.model.predict(sequence))\n",
    "            predicted_word = \"\"\n",
    "\n",
    "            for key, value in self.tokenizer.word_index.items():\n",
    "                if value == preds:\n",
    "                    predicted_word = key\n",
    "                    break\n",
    "\n",
    "            \n",
    "            sentiment = TextBlob(predicted_word).sentiment\n",
    "\n",
    "            sentiment_label = f\"Sentiment - Polarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}\"\n",
    "            self.result_label.config(text=f\"Predicted next word: {predicted_word}\\n{sentiment_label}\")\n",
    "            \n",
    "            \n",
    "            self.translate_btn.config(state=tk.NORMAL)\n",
    "            self.predicted_word = predicted_word  \n",
    "\n",
    "    def translate_word(self):\n",
    "        if hasattr(self, 'predicted_word'):\n",
    "            translated_word = self.translator.translate(self.predicted_word)\n",
    "            self.result_label.config(text=f\"Predicted next word: {self.predicted_word}\\nTranslation in Telugu: {translated_word}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = TextPredictorApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222f30ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 426ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import font as tkfont\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from textblob import TextBlob\n",
    "from translate import Translator\n",
    "\n",
    "class TextPredictorApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Text Predictor\")\n",
    "        \n",
    "        \n",
    "        self.root.geometry(\"500x300\")\n",
    "        self.root.configure(bg=\"#ffffff\")  \n",
    "\n",
    "        \n",
    "        self.custom_font = tkfont.Font(family=\"Arial\", size=14)\n",
    "\n",
    "        \n",
    "        self.create_widgets()\n",
    "\n",
    "        \n",
    "        self.model = load_model('next_words.h5')\n",
    "        with open('token.pkl', 'rb') as token_file:\n",
    "            self.tokenizer = pickle.load(token_file)\n",
    "        self.translator = Translator(to_lang=\"te\")\n",
    "\n",
    "    def create_widgets(self):\n",
    "        \n",
    "        self.header_frame = tk.Frame(self.root, bg=\"#ffffff\")\n",
    "        self.header_frame.pack(pady=20)\n",
    "\n",
    "        self.google_logo = tk.Label(self.header_frame, text=\"Google\", font=(\"Arial\", 24, \"bold\"), bg=\"#ffffff\", fg=\"#4285F4\")\n",
    "        self.google_logo.pack()\n",
    "\n",
    "        self.entry_label = tk.Label(self.root, text=\"Enter your text:\", font=self.custom_font, bg=\"#ffffff\")\n",
    "        self.entry_label.pack(pady=(10, 5))\n",
    "\n",
    "        self.text_entry = tk.Entry(self.root, width=60, font=self.custom_font)\n",
    "        self.text_entry.pack(pady=(0, 15))\n",
    "\n",
    "        self.predict_btn = tk.Button(self.root, text=\"Predict Next Word\", command=self.predict_next_word, font=self.custom_font, bg=\"#4285F4\", fg=\"#ffffff\", relief=tk.FLAT)\n",
    "        self.predict_btn.pack(pady=5)\n",
    "\n",
    "        self.translate_btn = tk.Button(self.root, text=\"Translate to Telugu\", command=self.translate_word, state=tk.DISABLED, font=self.custom_font, bg=\"#34A853\", fg=\"#ffffff\", relief=tk.FLAT)\n",
    "        self.translate_btn.pack(pady=5)\n",
    "\n",
    "        self.result_label = tk.Label(self.root, text=\"\", font=self.custom_font, bg=\"#ffffff\")\n",
    "        self.result_label.pack(pady=(20, 0))\n",
    "\n",
    "    def predict_next_word(self):\n",
    "        text = self.text_entry.get().strip()\n",
    "        if text:\n",
    "            words = text.split()[-3:]\n",
    "            sequence = self.tokenizer.texts_to_sequences([words])[0]\n",
    "            sequence = np.array([sequence])\n",
    "\n",
    "            preds = np.argmax(self.model.predict(sequence))\n",
    "            predicted_word = \"\"\n",
    "\n",
    "            for key, value in self.tokenizer.word_index.items():\n",
    "                if value == preds:\n",
    "                    predicted_word = key\n",
    "                    break\n",
    "\n",
    "            sentiment = TextBlob(predicted_word).sentiment\n",
    "\n",
    "            sentiment_label = f\"Sentiment - Polarity: {sentiment.polarity:.2f}, Subjectivity: {sentiment.subjectivity:.2f}\"\n",
    "            self.result_label.config(text=f\"Predicted next word: {predicted_word}\\n{sentiment_label}\")\n",
    "\n",
    "            self.translate_btn.config(state=tk.NORMAL)\n",
    "            self.predicted_word = predicted_word  \n",
    "\n",
    "    def translate_word(self):\n",
    "        if hasattr(self, 'predicted_word'):\n",
    "            translated_word = self.translator.translate(self.predicted_word)\n",
    "            self.result_label.config(text=f\"Predicted next word: {self.predicted_word}\\nTranslation in Telugu: {translated_word}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = TextPredictorApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "724774d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import font as tkfont\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from textblob import TextBlob\n",
    "from translate import Translator\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class TextPredictorApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Text Predictor\")\n",
    "\n",
    "        self.root.geometry(\"600x600\")\n",
    "        self.root.configure(bg=\"#ffffff\")\n",
    "\n",
    "        self.custom_font = tkfont.Font(family=\"Arial\", size=14)\n",
    "\n",
    "        self.create_widgets()\n",
    "\n",
    "        self.model = load_model('next_words.h5')\n",
    "        with open('token.pkl', 'rb') as token_file:\n",
    "            self.tokenizer = pickle.load(token_file)\n",
    "        self.translator = Translator(to_lang=\"te\")\n",
    "\n",
    "    def create_widgets(self):\n",
    "        self.header_frame = tk.Frame(self.root, bg=\"#ffffff\")\n",
    "        self.header_frame.pack(pady=20)\n",
    "\n",
    "        self.google_logo = tk.Label(self.header_frame, text=\"Word Predictor\", font=(\"Arial\", 24, \"bold\"), bg=\"#ffffff\", fg=\"#4285F4\")\n",
    "        self.google_logo.pack()\n",
    "\n",
    "        self.entry_label = tk.Label(self.root, text=\"Enter your text:\", font=self.custom_font, bg=\"#ffffff\")\n",
    "        self.entry_label.pack(pady=(10, 5))\n",
    "\n",
    "        self.text_entry = tk.Entry(self.root, width=60, font=self.custom_font)\n",
    "        self.text_entry.pack(pady=(0, 15))\n",
    "\n",
    "        self.predict_btn = tk.Button(self.root, text=\"Predict Next Word\", command=self.predict_next_word, font=self.custom_font, bg=\"#4285F4\", fg=\"#ffffff\", relief=tk.FLAT)\n",
    "        self.predict_btn.pack(pady=5)\n",
    "\n",
    "        self.translate_btn = tk.Button(self.root, text=\"Translate to Telugu\", command=self.translate_word, state=tk.DISABLED, font=self.custom_font, bg=\"#34A853\", fg=\"#ffffff\", relief=tk.FLAT)\n",
    "        self.translate_btn.pack(pady=5)\n",
    "\n",
    "        self.result_label = tk.Label(self.root, text=\"\", font=self.custom_font, bg=\"#ffffff\")\n",
    "        self.result_label.pack(pady=(20, 0))\n",
    "\n",
    "        self.related_words_label = tk.Label(self.root, text=\"\", font=self.custom_font, bg=\"#ffffff\")\n",
    "        self.related_words_label.pack(pady=(20, 0))\n",
    "\n",
    "    def predict_next_word(self):\n",
    "        text = self.text_entry.get().strip()\n",
    "        if text:\n",
    "            words = text.split()[-3:]\n",
    "            sequence = self.tokenizer.texts_to_sequences([words])[0]\n",
    "            sequence = np.array([sequence])\n",
    "\n",
    "            preds = np.argmax(self.model.predict(sequence))\n",
    "            predicted_word = \"\"\n",
    "\n",
    "            for key, value in self.tokenizer.word_index.items():\n",
    "                if value == preds:\n",
    "                    predicted_word = key\n",
    "                    break\n",
    "\n",
    "            sentiment = TextBlob(predicted_word).sentiment\n",
    "\n",
    "            sentiment_label = f\"Sentiment - Polarity: {sentiment.polarity:.2f}, Subjectivity: {sentiment.subjectivity:.2f}\"\n",
    "            self.result_label.config(text=f\"Predicted next word: {predicted_word}\\n{sentiment_label}\")\n",
    "\n",
    "            self.translate_btn.config(state=tk.NORMAL)\n",
    "            self.predicted_word = predicted_word\n",
    "\n",
    "            related_words = self.scrape_related_words(predicted_word)\n",
    "            related_words_tfidf = self.get_related_words_tfidf(predicted_word, related_words)\n",
    "            related_words_sentiments = self.get_related_words_sentiments(related_words_tfidf)\n",
    "            self.display_related_words(related_words_sentiments)\n",
    "\n",
    "    def scrape_related_words(self, query):\n",
    "        search_url = f\"https://www.google.com/search?q={query}\"\n",
    "        response = requests.get(search_url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        search_results = soup.find_all(\"h3\")\n",
    "        related_words = [result.text for result in search_results]\n",
    "        return related_words\n",
    "\n",
    "    def get_related_words_tfidf(self, query, related_words):\n",
    "        documents = [query] + related_words\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "        cosine_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "\n",
    "        related_words_with_scores = [(word, score) for word, score in zip(related_words, cosine_similarities)]\n",
    "        related_words_with_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [word for word, _ in related_words_with_scores]\n",
    "\n",
    "    def get_related_words_sentiments(self, related_words):\n",
    "        sentiments = []\n",
    "        for word in related_words:\n",
    "            sentiment = TextBlob(word).sentiment\n",
    "            sentiments.append((word, sentiment.polarity, sentiment.subjectivity))\n",
    "        return sentiments\n",
    "\n",
    "    def display_related_words(self, related_words_sentiments):\n",
    "        text = \"Related words and their sentiments:\\n\"\n",
    "        for word, polarity, subjectivity in related_words_sentiments:\n",
    "            text += f\"{word} - Polarity: {polarity:.2f}, Subjectivity: {subjectivity:.2f}\\n\"\n",
    "        self.related_words_label.config(text=text)\n",
    "\n",
    "    def translate_word(self):\n",
    "        if hasattr(self, 'predicted_word'):\n",
    "            translated_word = self.translator.translate(self.predicted_word)\n",
    "            self.result_label.config(text=f\"Predicted next word: {self.predicted_word}\\nTranslation in Telugu: {translated_word}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = TextPredictorApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa5195cb-a5e9-41bd-9cdd-48b653cf3a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SpeechRecognitionNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting pyaudio\n",
      "  Downloading PyAudio-0.2.14-cp312-cp312-win_amd64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\suhai\\downloads\\anaconda\\lib\\site-packages (from SpeechRecognition) (2.32.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\suhai\\downloads\\anaconda\\lib\\site-packages (from SpeechRecognition) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suhai\\downloads\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suhai\\downloads\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suhai\\downloads\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suhai\\downloads\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.6.2)\n",
      "Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n",
      "   ---------------------------------------- 0.0/32.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/32.8 MB 9.6 MB/s eta 0:00:04\n",
      "   ---------------------------------------- 0.4/32.8 MB 5.7 MB/s eta 0:00:06\n",
      "    --------------------------------------- 0.7/32.8 MB 5.9 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.1/32.8 MB 6.9 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 1.4/32.8 MB 6.9 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.7/32.8 MB 6.9 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 2.0/32.8 MB 6.9 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.5/32.8 MB 7.3 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.7/32.8 MB 6.9 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.8/32.8 MB 6.5 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 3.2/32.8 MB 6.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 3.3/32.8 MB 6.2 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 3.4/32.8 MB 5.8 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 3.5/32.8 MB 5.6 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 3.5/32.8 MB 5.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 3.6/32.8 MB 5.0 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 3.7/32.8 MB 4.8 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 3.7/32.8 MB 4.7 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 3.8/32.8 MB 4.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 3.8/32.8 MB 4.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 3.9/32.8 MB 4.0 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 3.9/32.8 MB 3.9 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 3.9/32.8 MB 3.8 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 4.1/32.8 MB 3.7 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 4.2/32.8 MB 3.6 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 4.2/32.8 MB 3.5 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 4.3/32.8 MB 3.5 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 4.3/32.8 MB 3.5 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 4.3/32.8 MB 3.3 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 4.3/32.8 MB 3.1 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 4.4/32.8 MB 3.1 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 4.4/32.8 MB 3.0 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 4.4/32.8 MB 3.0 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 4.5/32.8 MB 2.9 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 4.6/32.8 MB 2.9 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 4.6/32.8 MB 2.9 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 4.6/32.8 MB 2.8 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 4.6/32.8 MB 2.8 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 4.7/32.8 MB 2.6 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 4.8/32.8 MB 2.6 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 4.8/32.8 MB 2.6 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 4.9/32.8 MB 2.5 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 5.0/32.8 MB 2.5 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 5.1/32.8 MB 2.5 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 5.1/32.8 MB 2.5 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 5.2/32.8 MB 2.4 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 5.2/32.8 MB 2.4 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 5.2/32.8 MB 2.4 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 5.3/32.8 MB 2.3 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 5.3/32.8 MB 2.3 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 5.3/32.8 MB 2.2 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 5.3/32.8 MB 2.2 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 5.4/32.8 MB 2.2 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 5.4/32.8 MB 2.2 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 5.5/32.8 MB 2.1 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 5.6/32.8 MB 2.1 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 5.8/32.8 MB 2.2 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 5.9/32.8 MB 2.2 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 6.2/32.8 MB 2.3 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 6.6/32.8 MB 2.4 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 6.8/32.8 MB 2.4 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 7.2/32.8 MB 2.5 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 7.6/32.8 MB 2.6 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 7.9/32.8 MB 2.6 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 8.2/32.8 MB 2.7 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 8.4/32.8 MB 2.7 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 8.5/32.8 MB 2.7 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 8.9/32.8 MB 2.8 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 9.2/32.8 MB 2.9 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 9.4/32.8 MB 2.9 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 9.8/32.8 MB 3.0 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 10.1/32.8 MB 3.0 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 10.4/32.8 MB 3.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 10.7/32.8 MB 3.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 11.1/32.8 MB 3.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 11.4/32.8 MB 3.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 11.7/32.8 MB 3.0 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 12.1/32.8 MB 3.0 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 12.4/32.8 MB 3.0 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 12.7/32.8 MB 3.0 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 13.0/32.8 MB 3.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 13.4/32.8 MB 3.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 13.8/32.8 MB 3.2 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 14.2/32.8 MB 3.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 14.4/32.8 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 14.9/32.8 MB 4.6 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 15.2/32.8 MB 5.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 15.5/32.8 MB 5.6 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 15.8/32.8 MB 6.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 16.1/32.8 MB 6.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 16.5/32.8 MB 6.8 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 16.9/32.8 MB 6.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 17.3/32.8 MB 7.0 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 17.5/32.8 MB 6.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 17.8/32.8 MB 6.8 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 18.0/32.8 MB 6.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 18.5/32.8 MB 6.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 18.8/32.8 MB 7.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 19.2/32.8 MB 7.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 19.6/32.8 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 20.1/32.8 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 20.5/32.8 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 20.9/32.8 MB 7.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 21.4/32.8 MB 7.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 21.8/32.8 MB 7.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 22.2/32.8 MB 7.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 22.5/32.8 MB 7.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 22.9/32.8 MB 7.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 23.3/32.8 MB 7.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 23.7/32.8 MB 7.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 24.1/32.8 MB 7.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 24.6/32.8 MB 8.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 25.0/32.8 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 25.4/32.8 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 25.8/32.8 MB 8.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 26.3/32.8 MB 8.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 26.7/32.8 MB 8.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 27.2/32.8 MB 8.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 27.6/32.8 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 28.0/32.8 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 28.5/32.8 MB 9.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 28.8/32.8 MB 9.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 29.3/32.8 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 29.6/32.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 30.1/32.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 30.5/32.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 30.9/32.8 MB 9.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 31.3/32.8 MB 8.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 31.7/32.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.2/32.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.5/32.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.8/32.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 32.8/32.8 MB 8.5 MB/s eta 0:00:00\n",
      "Downloading PyAudio-0.2.14-cp312-cp312-win_amd64.whl (164 kB)\n",
      "   ---------------------------------------- 0.0/164.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 164.1/164.1 kB 9.6 MB/s eta 0:00:00\n",
      "Installing collected packages: pyaudio, SpeechRecognition\n",
      "Successfully installed SpeechRecognition-3.10.4 pyaudio-0.2.14\n"
     ]
    }
   ],
   "source": [
    "pip install SpeechRecognition pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3353f077-9fe9-4086-80e0-9487167ac1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import font as tkfont\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from textblob import TextBlob\n",
    "from translate import Translator\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import speech_recognition as sr\n",
    "\n",
    "class TextPredictorApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Text Predictor\")\n",
    "\n",
    "        self.root.geometry(\"600x600\")\n",
    "        self.root.configure(bg=\"#ffffff\")\n",
    "\n",
    "        self.custom_font = tkfont.Font(family=\"Arial\", size=14)\n",
    "\n",
    "        self.create_widgets()\n",
    "\n",
    "        self.model = load_model('next_words.h5')\n",
    "        with open('token.pkl', 'rb') as token_file:\n",
    "            self.tokenizer = pickle.load(token_file)\n",
    "        self.translator = Translator(to_lang=\"te\")\n",
    "\n",
    "    def create_widgets(self):\n",
    "        self.header_frame = tk.Frame(self.root, bg=\"#ffffff\")\n",
    "        self.header_frame.pack(pady=20)\n",
    "\n",
    "        self.google_logo = tk.Label(self.header_frame, text=\"Word Predictor\", font=(\"Arial\", 24, \"bold\"), bg=\"#ffffff\", fg=\"#4285F4\")\n",
    "        self.google_logo.pack()\n",
    "\n",
    "        self.entry_label = tk.Label(self.root, text=\"Enter your text or use voice input:\", font=self.custom_font, bg=\"#ffffff\")\n",
    "        self.entry_label.pack(pady=(10, 5))\n",
    "\n",
    "        self.text_entry = tk.Entry(self.root, width=60, font=self.custom_font)\n",
    "        self.text_entry.pack(pady=(0, 15))\n",
    "\n",
    "        self.voice_btn = tk.Button(self.root, text=\"Voice Input\", command=self.voice_input, font=self.custom_font, bg=\"#FBBC05\", fg=\"#ffffff\", relief=tk.FLAT)\n",
    "        self.voice_btn.pack(pady=5)\n",
    "\n",
    "        self.predict_btn = tk.Button(self.root, text=\"Predict Next Word\", command=self.predict_next_word, font=self.custom_font, bg=\"#4285F4\", fg=\"#ffffff\", relief=tk.FLAT)\n",
    "        self.predict_btn.pack(pady=5)\n",
    "\n",
    "        self.translate_btn = tk.Button(self.root, text=\"Translate to Telugu\", command=self.translate_word, state=tk.DISABLED, font=self.custom_font, bg=\"#34A853\", fg=\"#ffffff\", relief=tk.FLAT)\n",
    "        self.translate_btn.pack(pady=5)\n",
    "\n",
    "        self.result_label = tk.Label(self.root, text=\"\", font=self.custom_font, bg=\"#ffffff\")\n",
    "        self.result_label.pack(pady=(20, 0))\n",
    "\n",
    "        self.related_words_label = tk.Label(self.root, text=\"\", font=self.custom_font, bg=\"#ffffff\")\n",
    "        self.related_words_label.pack(pady=(20, 0))\n",
    "\n",
    "    def voice_input(self):\n",
    "        recognizer = sr.Recognizer()\n",
    "        with sr.Microphone() as source:\n",
    "            self.result_label.config(text=\"Listening...\")\n",
    "            audio = recognizer.listen(source)\n",
    "            try:\n",
    "                text = recognizer.recognize_google(audio)\n",
    "                self.text_entry.delete(0, tk.END)\n",
    "                self.text_entry.insert(0, text)\n",
    "                self.result_label.config(text=\"Voice input received. You can now predict the next word.\")\n",
    "            except sr.UnknownValueError:\n",
    "                self.result_label.config(text=\"Sorry, could not understand the audio.\")\n",
    "            except sr.RequestError:\n",
    "                self.result_label.config(text=\"Could not request results; check your network connection.\")\n",
    "\n",
    "    def predict_next_word(self):\n",
    "        text = self.text_entry.get().strip()\n",
    "        if text:\n",
    "            words = text.split()[-3:]\n",
    "            sequence = self.tokenizer.texts_to_sequences([words])[0]\n",
    "            sequence = np.array([sequence])\n",
    "\n",
    "            preds = np.argmax(self.model.predict(sequence))\n",
    "            predicted_word = \"\"\n",
    "\n",
    "            for key, value in self.tokenizer.word_index.items():\n",
    "                if value == preds:\n",
    "                    predicted_word = key\n",
    "                    break\n",
    "\n",
    "            sentiment = TextBlob(predicted_word).sentiment\n",
    "\n",
    "            sentiment_label = f\"Sentiment - Polarity: {sentiment.polarity:.2f}, Subjectivity: {sentiment.subjectivity:.2f}\"\n",
    "            self.result_label.config(text=f\"Predicted next word: {predicted_word}\\n{sentiment_label}\")\n",
    "\n",
    "            self.translate_btn.config(state=tk.NORMAL)\n",
    "            self.predicted_word = predicted_word\n",
    "\n",
    "            related_words = self.scrape_related_words(predicted_word)\n",
    "            related_words_tfidf = self.get_related_words_tfidf(predicted_word, related_words)\n",
    "            related_words_sentiments = self.get_related_words_sentiments(related_words_tfidf)\n",
    "            self.display_related_words(related_words_sentiments)\n",
    "\n",
    "    def scrape_related_words(self, query):\n",
    "        search_url = f\"https://www.google.com/search?q={query}\"\n",
    "        response = requests.get(search_url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        search_results = soup.find_all(\"h3\")\n",
    "        related_words = [result.text for result in search_results]\n",
    "        return related_words\n",
    "\n",
    "    def get_related_words_tfidf(self, query, related_words):\n",
    "        documents = [query] + related_words\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "        cosine_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "\n",
    "        related_words_with_scores = [(word, score) for word, score in zip(related_words, cosine_similarities)]\n",
    "        related_words_with_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [word for word, _ in related_words_with_scores]\n",
    "\n",
    "    def get_related_words_sentiments(self, related_words):\n",
    "        sentiments = []\n",
    "        for word in related_words:\n",
    "            sentiment = TextBlob(word).sentiment\n",
    "            sentiments.append((word, sentiment.polarity, sentiment.subjectivity))\n",
    "        return sentiments\n",
    "\n",
    "    def display_related_words(self, related_words_sentiments):\n",
    "        text = \"Related words and their sentiments:\\n\"\n",
    "        for word, polarity, subjectivity in related_words_sentiments:\n",
    "            text += f\"{word} - Polarity: {polarity:.2f}, Subjectivity: {subjectivity:.2f}\\n\"\n",
    "        self.related_words_label.config(text=text)\n",
    "\n",
    "    def translate_word(self):\n",
    "        if hasattr(self, 'predicted_word'):\n",
    "            translated_word = self.translator.translate(self.predicted_word)\n",
    "            self.result_label.config(text=f\"Predicted next word: {self.predicted_word}\\nTranslation in Telugu: {translated_word}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = TextPredictorApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5396a19-af0b-499b-afa4-81fbaefd9b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyttsx3\n",
      "  Downloading pyttsx3-2.90-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting comtypes (from pyttsx3)\n",
      "  Downloading comtypes-1.4.5-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting pypiwin32 (from pyttsx3)\n",
      "  Downloading pypiwin32-223-py3-none-any.whl.metadata (236 bytes)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\suhai\\downloads\\anaconda\\lib\\site-packages (from pyttsx3) (305.1)\n",
      "Downloading pyttsx3-2.90-py3-none-any.whl (39 kB)\n",
      "Downloading comtypes-1.4.5-py3-none-any.whl (219 kB)\n",
      "   ---------------------------------------- 0.0/219.4 kB ? eta -:--:--\n",
      "   ---------------------------------------  215.0/219.4 kB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 219.4/219.4 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading pypiwin32-223-py3-none-any.whl (1.7 kB)\n",
      "Installing collected packages: pypiwin32, comtypes, pyttsx3\n",
      "Successfully installed comtypes-1.4.5 pypiwin32-223 pyttsx3-2.90\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e222c85-8b54-41ca-950d-e51df89a3195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import font as tkfont\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from textblob import TextBlob\n",
    "from translate import Translator\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "\n",
    "class TextPredictorApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Text Predictor\")\n",
    "\n",
    "        self.root.geometry(\"600x600\")\n",
    "        self.root.configure(bg=\"#ffffff\")\n",
    "\n",
    "        self.custom_font = tkfont.Font(family=\"Arial\", size=14)\n",
    "\n",
    "        self.create_widgets()\n",
    "\n",
    "        self.model = load_model('next_words.h5')\n",
    "        with open('token.pkl', 'rb') as token_file:\n",
    "            self.tokenizer = pickle.load(token_file)\n",
    "        self.translator = Translator(to_lang=\"te\")\n",
    "\n",
    "        self.engine = pyttsx3.init()\n",
    "\n",
    "    def create_widgets(self):\n",
    "        self.header_frame = tk.Frame(self.root, bg=\"#ffffff\")\n",
    "        self.header_frame.pack(pady=20)\n",
    "\n",
    "        self.google_logo = tk.Label(self.header_frame, text=\"Word Predictor\", font=(\"Arial\", 24, \"bold\"), bg=\"#ffffff\", fg=\"#4285F4\")\n",
    "        self.google_logo.pack()\n",
    "\n",
    "        self.entry_label = tk.Label(self.root, text=\"Enter your text or use voice input:\", font=self.custom_font, bg=\"#ffffff\")\n",
    "        self.entry_label.pack(pady=(10, 5))\n",
    "\n",
    "        self.text_entry = tk.Entry(self.root, width=60, font=self.custom_font)\n",
    "        self.text_entry.pack(pady=(0, 15))\n",
    "\n",
    "        self.voice_btn = tk.Button(self.root, text=\"Voice Input\", command=self.voice_input, font=self.custom_font, bg=\"#FBBC05\", fg=\"#ffffff\", relief=tk.FLAT)\n",
    "        self.voice_btn.pack(pady=5)\n",
    "\n",
    "        self.predict_btn = tk.Button(self.root, text=\"Predict Next Word\", command=self.predict_next_word, font=self.custom_font, bg=\"#4285F4\", fg=\"#ffffff\", relief=tk.FLAT)\n",
    "        self.predict_btn.pack(pady=5)\n",
    "\n",
    "        self.translate_btn = tk.Button(self.root, text=\"Translate to Telugu\", command=self.translate_word, state=tk.DISABLED, font=self.custom_font, bg=\"#34A853\", fg=\"#ffffff\", relief=tk.FLAT)\n",
    "        self.translate_btn.pack(pady=5)\n",
    "\n",
    "        self.read_output_btn = tk.Button(self.root, text=\"Read Output\", command=self.read_output, state=tk.DISABLED, font=self.custom_font, bg=\"#EA4335\", fg=\"#ffffff\", relief=tk.FLAT)\n",
    "        self.read_output_btn.pack(pady=5)\n",
    "\n",
    "        self.result_label = tk.Label(self.root, text=\"\", font=self.custom_font, bg=\"#ffffff\")\n",
    "        self.result_label.pack(pady=(20, 0))\n",
    "\n",
    "        self.related_words_label = tk.Label(self.root, text=\"\", font=self.custom_font, bg=\"#ffffff\")\n",
    "        self.related_words_label.pack(pady=(20, 0))\n",
    "\n",
    "    def voice_input(self):\n",
    "        recognizer = sr.Recognizer()\n",
    "        with sr.Microphone() as source:\n",
    "            self.result_label.config(text=\"Listening...\")\n",
    "            audio = recognizer.listen(source)\n",
    "            try:\n",
    "                text = recognizer.recognize_google(audio)\n",
    "                self.text_entry.delete(0, tk.END)\n",
    "                self.text_entry.insert(0, text)\n",
    "                self.result_label.config(text=\"Voice input received. You can now predict the next word.\")\n",
    "            except sr.UnknownValueError:\n",
    "                self.result_label.config(text=\"Sorry, could not understand the audio.\")\n",
    "            except sr.RequestError:\n",
    "                self.result_label.config(text=\"Could not request results; check your network connection.\")\n",
    "\n",
    "    def predict_next_word(self):\n",
    "        text = self.text_entry.get().strip()\n",
    "        if text:\n",
    "            words = text.split()[-3:]\n",
    "            sequence = self.tokenizer.texts_to_sequences([words])[0]\n",
    "            sequence = np.array([sequence])\n",
    "\n",
    "            preds = np.argmax(self.model.predict(sequence))\n",
    "            predicted_word = \"\"\n",
    "\n",
    "            for key, value in self.tokenizer.word_index.items():\n",
    "                if value == preds:\n",
    "                    predicted_word = key\n",
    "                    break\n",
    "\n",
    "            sentiment = TextBlob(predicted_word).sentiment\n",
    "\n",
    "            sentiment_label = f\"Sentiment - Polarity: {sentiment.polarity:.2f}, Subjectivity: {sentiment.subjectivity:.2f}\"\n",
    "            self.result_label.config(text=f\"Predicted next word: {predicted_word}\\n{sentiment_label}\")\n",
    "\n",
    "            self.translate_btn.config(state=tk.NORMAL)\n",
    "            self.read_output_btn.config(state=tk.NORMAL)\n",
    "            self.predicted_word = predicted_word\n",
    "\n",
    "            related_words = self.scrape_related_words(predicted_word)\n",
    "            related_words_tfidf = self.get_related_words_tfidf(predicted_word, related_words)\n",
    "            related_words_sentiments = self.get_related_words_sentiments(related_words_tfidf)\n",
    "            self.display_related_words(related_words_sentiments)\n",
    "\n",
    "    def scrape_related_words(self, query):\n",
    "        search_url = f\"https://www.google.com/search?q={query}\"\n",
    "        response = requests.get(search_url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        search_results = soup.find_all(\"h3\")\n",
    "        related_words = [result.text for result in search_results]\n",
    "        return related_words\n",
    "\n",
    "    def get_related_words_tfidf(self, query, related_words):\n",
    "        documents = [query] + related_words\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "        cosine_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "\n",
    "        related_words_with_scores = [(word, score) for word, score in zip(related_words, cosine_similarities)]\n",
    "        related_words_with_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [word for word, _ in related_words_with_scores]\n",
    "\n",
    "    def get_related_words_sentiments(self, related_words):\n",
    "        sentiments = []\n",
    "        for word in related_words:\n",
    "            sentiment = TextBlob(word).sentiment\n",
    "            sentiments.append((word, sentiment.polarity, sentiment.subjectivity))\n",
    "        return sentiments\n",
    "\n",
    "    def display_related_words(self, related_words_sentiments):\n",
    "        text = \"Related words and their sentiments:\\n\"\n",
    "        for word, polarity, subjectivity in related_words_sentiments:\n",
    "            text += f\"{word} - Polarity: {polarity:.2f}, Subjectivity: {subjectivity:.2f}\\n\"\n",
    "        self.related_words_label.config(text=text)\n",
    "\n",
    "    def translate_word(self):\n",
    "        if hasattr(self, 'predicted_word'):\n",
    "            translated_word = self.translator.translate(self.predicted_word)\n",
    "            self.result_label.config(text=f\"Predicted next word: {self.predicted_word}\\nTranslation in Telugu: {translated_word}\")\n",
    "\n",
    "    def read_output(self):\n",
    "        output_text = self.result_label.cget(\"text\") + \"\\n\" + self.related_words_label.cget(\"text\")\n",
    "        self.engine.say(output_text)\n",
    "        self.engine.runAndWait()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = TextPredictorApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63c0a0-f8ac-41a0-9e5b-85b049e18103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
